{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad3fb14",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The project was part of Udacity’s Data Scientist nanodegree and is one of the most popular Udacity projects across machine learning and artificial intelligence nanodegree programs.\n",
    "\n",
    "The aim of the project in the Data Scientist nanodegree is to create a web application that is able to identify a breed of dog if given a photo or image as input. If the photo or image contains a human face (or alien face), then the application will return the breed of dog that most resembles this person or alien (see Chewbacca in the photo above).\n",
    "\n",
    "The strategy laid out for solving this problem, given in the notebook provided by Udacity, is as follows:\n",
    "\n",
    "    Step 0: Import Datasets\n",
    "    Step 1: Detect Humans\n",
    "    Step 2: Detect Dogs\n",
    "    Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "    Step 4: Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "    Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "    Step 6: Write your Algorithm\n",
    "    Step 7: Test Your Algorithm\n",
    "\n",
    "In this project we have used Keras to build our Convolutional Neural Network (CNN) to make the dog predictions, though it would also be possible to use PyTorch, which we have used in a previous project in this Data Science nanodegree to classify flower species from a given image.\n",
    "\n",
    "Ideally, we would like to create a CNN that can achieve results of over 90%, that is it correctly identifies the dog breed 9 times out of 10. Though the criteria set by Udacity was at least 60%. We will be using the accuracy metric on the testing dataset to measure the performance of our models.\n",
    "\n",
    "To follow along with the steps you can download or clone the notebook from my repository on github here.\n",
    "\n",
    "Step 0: Import Datasets\n",
    "\n",
    "The datasets are provided by Udacity through the following links.\n",
    "\n",
    "    dog images for training the models\n",
    "    human faces for detector\n",
    "    all other downloads to ensure smooth running of the notebook are available in the repository.\n",
    "\n",
    "The first thing we do is load all the libraries and packages that have been used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a778551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.23.4-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.13.0-py2.py3-none-any.whl (174 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.10.0-py3-none-any.whl (6.2 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyparsing, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, packaging, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.9.24 charset-normalizer-2.1.1 flatbuffers-22.10.26 gast-0.4.0 google-auth-2.13.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.50.0 h5py-3.7.0 idna-3.4 importlib-metadata-5.0.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 numpy-1.23.4 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 setuptools-65.5.0 six-1.16.0 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.0.1 typing-extensions-4.4.0 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.4 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.23.4 which is incompatible.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.2 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.13.0 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.13.0 which is incompatible.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.13.0 which is incompatible.\n",
      "astroid 2.6.6 requires wrapt<1.13,>=1.11, but you have wrapt 1.14.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# import libraries for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline \n",
    "\n",
    "!pip install keras\n",
    "!pip install --ignore-installed --upgrade tensorflow\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions \n",
    "from keras.preprocessing import image                  \n",
    "\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f61c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\ayour\\anaconda3\\lib\\site-packages (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/paulstancliffe/Dog-Breed-Classifier/blob/master/dog_app_v2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c17a27",
   "metadata": {},
   "source": [
    "As you can see, we have used extensively from keras for creating the CNN, we have also used sklearn for dataset loading, OpenCV and PIL for image work, matplotlib for viewing the images and numpy for processing tensors.\n",
    "\n",
    "tqdm provides a smart progress meter so you can see how your for loops are progressing and glob is used to find all pathnames matching a specified pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6edb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../../../data/dog_images/train')\n",
    "valid_files, valid_targets = load_dataset('../../../data/dog_images/valid')\n",
    "test_files, test_targets = load_dataset('../../../data/dog_images/test')# load list of dog names\n",
    "dog_names = [item[35:-1] for item in sorted(glob(\"../../../data/dog_images/train/*/\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f01aba",
   "metadata": {},
   "source": [
    "After loading our libraries, we use the load dataset function from sklearn to import our datasets for our dog breed model training.\n",
    "\n",
    "The dog_names variable stores a list of the names for the classes which we will use in our final prediction model. Depending on the path for where you have the images you may need to change the 35 in the item[35:-1] to a bigger or smaller number.\n",
    "\n",
    "If everything has worked you will see 133 different dog breeds and ~8.3k dog images.\n",
    "\n",
    "# Step 1: Detect Humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"../../../data/lfw/*/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751ceaa",
   "metadata": {},
   "source": [
    "We use OpenCV’s implementation of Haar feature-based cascade classifiers to detect human faces in images. OpenCV provides many pre-trained face detectors. The code below instantiates the Haar Cascade Classifier from OpenCV and is then used in the face detector function to determine if a supplied image contains a face or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c071531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1608cb",
   "metadata": {},
   "source": [
    "    Number of humans correctly identified: 100\n",
    "    Number of dogs recognised has humans: 11\n",
    "\n",
    "The results are not perfect but acceptable.\n",
    "\n",
    "# Step 2: Detect Dogs\n",
    "\n",
    "For the dog detector we have used the pretrained Resnet50 network. The weights used were the standard ones for the dataset imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ded912",
   "metadata": {},
   "source": [
    "To use this model with our images, we need to process our images into the correct tensor size for the model. This is often one of the most challenging parts in the image classification process, the preprocessing of the images.\n",
    "\n",
    "For keras, the images need to be in four dimensions, formatted like this (number of images, row size of image in pixels, column size of image in pixels, number of color channels).\n",
    "\n",
    "In the path_to_tensor function we are processing a single image, so the output is (1,224,224,3), where 1 image, 224 pixels wide, 224 pixels high, and 3 colours red, green and blue. The image is loaded using the PIL library, and converted to the size 224x224. the img_to_array method separates the colors to (224x224x3) and finally we add a dimension at the front using the numpy expand_dims function to obtain our (1,224,224,3).\n",
    "\n",
    "The function paths_to_tensor then stacks the images returned from path_to_tensor into a 4D tensor with the number of images from training, validation or test datasets depending on which img_path is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that process images before sending to CNNdef path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea222b",
   "metadata": {},
   "source": [
    "One final preprocessing step is made by a built-in function in keras called preprocess_input. This function takes the output from image processing steps above and then reverses the colors to blue green red, which is the order keras expects and then normalizes the pixels based on standards for use with pretrained imagenet models. The exact numbers for this can be seen in the notebook.\n",
    "\n",
    "Now we are ready to make predictions. The function shown below, after completing the preprocessing steps above, uses the predict function to obtain an array for imagenet’s 1000 classes. We then use numpy’s argmax function to isolate the class with the highest probability and use imagenet’s dictionary to identify the name of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d68828",
   "metadata": {},
   "source": [
    "You will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151–268, and this explains the return on the dog_detector function below. If the prediction is within this range (151 to 268), return True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb69477",
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa0a26",
   "metadata": {},
   "source": [
    "# Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "This is a very interesting exercise to do and although it is not used in the final web application it is very useful for understanding CNNs and how they work. We will be utilising transfer learning for the final image detector.\n",
    "\n",
    "The full code is in the notebook and you can follow along experimenting and creating your own code from scratch.\n",
    "\n",
    "The network I chose was close to that presented in the class which consisted of 3 convolutional layers with 3 max pooling layers to reduce the dimensionality and increase the depth. The filters used were 16, 32, 64 respectively.\n",
    "\n",
    "I added a couple of fully connected layers with the final layer having 133 nodes to match our classes of dog breeds and a softmax activation function to obtain probabilities for each of the classes.\n",
    "\n",
    "Dropouts were added to reduce the possibility of overfitting. The default settings with Adam were used as the optimizer for the loss function.\n",
    "\n",
    "The target was to achieve a CNN with >1% accuracy. The network described above achieved 8.6% without any fine-tuning of parameters and without any augmentation on the data.\n",
    "\n",
    "# Step 4: Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "In this section of the Jupyter notebook, we are walked through using one of the pretrained networks available for use with keras.\n",
    "\n",
    "The most important concept to understand, especially if you have been used to using PyTorch for transfer learning is the use of bottleneck features.\n",
    "\n",
    "Bottleneck features is the concept of taking a pre-trained model in our case here VGG16 and chopping off the top classifying layer, and then inputing this “chopped” VGG16 as the first layer into our model.\n",
    "\n",
    "The bottleneck features are the last activation maps in the VGG16, (the fully-connected layers for classifying has been cut off) thus making it now an effective feature extractor.\n",
    "\n",
    "I haven’t included any code here as we will follow the same process in step 5 with the Resnet50 pretrained model used for transfer learning. Check out the code in the Jupyter notebook.\n",
    "\n",
    "# Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "Ok, so let’s build our model in keras.\n",
    "\n",
    "I decided to use the Resnet50 model, as I have used it in the past both with PyTorch and Fast.ai and it has proved to be both accurate and relatively quick with training.\n",
    "\n",
    "Udacity has prepared in advance the extraction of bottleneck features for each of the pretrained networks on the dog images. So it is only necessary to download the file for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_resnet50 = bottleneck_features['train']\n",
    "valid_resnet50 = bottleneck_features['valid']\n",
    "test_resnet50 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d35a8",
   "metadata": {},
   "source": [
    "The above variables contain images that have already been put through the bottleneck extractor. This will make the training of our model very quick, as we are applying images where the main features for identification have already been isolated. This means we will have only a small number of parameters or weights to backpropogate through.\n",
    "\n",
    "Our fully-connected layers will now just correlate the patterns for our 133 classes coming from the bottleneck features in order to train, validate and test.\n",
    "\n",
    "The shape of train_resnet50 of (6680,1,1,2048) and we use this has the input shape into the first layer of our model, shown by the GlobalAveragePooling2D(input_shape=train_resnet50.shape[1:]), the index is [1:] as the number of images(6680) is not an input to the neural network.\n",
    "\n",
    "The model architecture is shown below. I tried several different combinations with 1 and 2 fully connected layers with 512 and 1024 nodes, different rates of dropout(0.15,0.25 and 0.4) and different optimizers (SGD, Adam, rmsprop). All the different architectures gave me testing accuracies of between 81% and 85%.\n",
    "\n",
    "The final layer is 133 nodes to match the number of our classes.\n",
    "\n",
    "The architecture below has a testing accuracy of 84.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your architecture.resnet50_model = Sequential()\n",
    "resnet50_model.add(GlobalAveragePooling2D(input_shape=train_resnet50.shape[1:]))\n",
    "resnet50_model.add(Dense(1024, activation='relu'))\n",
    "resnet50_model.add(Dropout(0.4))\n",
    "resnet50_model.add(Dense(133, activation='softmax'))resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756fb6a",
   "metadata": {},
   "source": [
    "The total trainable parameters or weights for this model is 2.2 million. We then compile the model, i.e select loss optimization function and loss measurement, SGD and categorical_crossentropy respectively, in our case and indicate what performance metric we would like to score the model with. We are going to use the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "resnet50_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46693348",
   "metadata": {},
   "source": [
    "Next, we train our specified model with the fit function, validating it for backpropagation updating of parameters. Set the number of epochs to run through all the training images and how many images to train at a time through the batch size and finally we use the callback parameter to save our model whilst training for the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.resnet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)resnet50_model.fit(train_resnet50, train_targets, \n",
    "          validation_data=(valid_resnet50, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183fde1",
   "metadata": {},
   "source": [
    "And voila, we load back our best parameters and we have a trained model for classifying breeds of dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f82a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights with the best validation loss.\n",
    "resnet50_model.load_weights('saved_models/weights.best.resnet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4a661",
   "metadata": {},
   "source": [
    "We test this to check its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "resnet50_predictions = [np.argmax(resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_resnet50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a118f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(resnet50_predictions)==np.argmax(test_targets, axis=1))/len(resnet50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f824e9",
   "metadata": {},
   "source": [
    "Test accuracy: 84.8086%\n",
    "\n",
    "The next step is implementing the model into a function that can be used in our web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *def resnet50_predict_breed(img_path):\n",
    "    \"\"\"\n",
    "    INPUT: path to an image\n",
    "    OUTPUT: returns a prediction of dog breed\n",
    "    \"\"\"\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    \n",
    "    # obtain predicted vector\n",
    "    predicted_vector = resnet50_model.predict(bottleneck_feature)\n",
    "    \n",
    "    # get class with highest probability and match to label for class\n",
    "    predicted_index = np.argmax(predicted_vector)\n",
    "    label = dog_names[predicted_index]\n",
    "    \n",
    "    return label #predicted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2995e",
   "metadata": {},
   "source": [
    "We input an image path, the bottleneck features for our pretrained model are applied to the image, this is then processed through our trained fully-connected model to give a prediction vector.\n",
    "\n",
    "To this we apply the numpy argmax function to extract the highest probability class/index and use our labels right from the beginning of the notebook to get the name of the dog breed.\n",
    "\n",
    "# Step 6: Write your Algorithm\n",
    "\n",
    "Here we are creating our algorithm to analyze any image. The algorithm accepts a file path and:\n",
    "\n",
    "    if a dog is detected in the image, return the predicted breed.\n",
    "    if a human is detected in the image, return the resembling dog breed.\n",
    "    if neither is detected in the image, provide output that indicates an error.\n",
    "\n",
    "The algorithm collects together functions we used previously to create a final output and shows the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ad85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(img_path):\n",
    "    \"\"\"\n",
    "    This function takes in an image and returns a prediction on dog breed.\n",
    "    INPUT: the path to the image to be classified\n",
    "    OUTPUT: returns either dog breed, human dog breed or neither dog or human\n",
    "    \"\"\"\n",
    "    # check if image is a dog\n",
    "    dog = dog_detector(img_path)\n",
    "    \n",
    "    # check if image contains a human face\n",
    "    human = face_detector(img_path)\n",
    "    \n",
    "    # make a prediction of dog_breed based on image\n",
    "    dog_breed = resnet50_predict_breed(img_path)\n",
    "    \n",
    "    # plot image with comment\n",
    "    img = cv2.imread(img_path)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(cv_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "    if dog:\n",
    "        return(\"This photo looks like a {}\".format(dog_breed))\n",
    "    elif human:\n",
    "        return(\"This human resembles a {}\".format(dog_breed))\n",
    "    else:\n",
    "        return(\"This is neither dog beast or human!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110298c",
   "metadata": {},
   "source": [
    "# Step 7: Test Your Algorithm\n",
    "\n",
    "The results were pretty good for the images the model was shown. Only Chewbacca from Star Wars was identified as a dog and given a dog breed. All the other images were correctly identified, but a more robust testing of dog breeds would be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963921c",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "At the start of the article our objective was to create a CNN with 90% testing accuracy. Our final model obtained only 85% testing accuracy.\n",
    "\n",
    "However, given more time, (I’m working against Udacity’s Project Deadlines!) I would have experimented with the ImageDataGenerator class in keras (link in the references below) to augment the images.\n",
    "\n",
    "I would have done some hyperparameter fine tuning with the optimizer. For example SGD has the following parameters.\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False).\n",
    "Lowering the learning rate with a Learning Rate Scheduler, using weight decay, momentum and nesterov would all likely improve the accuracy of our model.\n",
    "\n",
    "We could also have looked more deeply into the images themselves that we used for training the dog breeds. We could have looked at a confusion matrix to see which images were giving the biggest errors in the validation data in order to identify possible noise. Maybe some of these images were too blurred and the model was generalizing well but being deceived by unclear images.\n",
    "\n",
    "We could check the training images with random sampling to see the quality of the images and delete images that were badly focused or with more than one breed of dog, ie reduce noise in the training data.\n",
    "\n",
    "We could check to see if there were sufficient training image of each breed of dog and that the image classes were balanced overall in terms of training numbers.\n",
    "\n",
    "Following the above areas I’m sure we could increase the testing accuracy of the model to above 90%.\n",
    "\n",
    "To summarise possible improvements are:\n",
    "\n",
    "    analysis of images used to train and validate the model\n",
    "    data augmentation\n",
    "    fine tune hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
